{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkvTm5USX8f-",
        "outputId": "3cde61e3-0dcc-405f-a8d3-1928cc723cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/abhyudaya12/veri-vehicle-re-identification-dataset\n",
            "License(s): Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\n",
            "Downloading veri-vehicle-re-identification-dataset.zip to /content/veri-vehicle-re-identification-dataset\n",
            " 96% 905M/946M [00:04<00:00, 155MB/s]\n",
            "100% 946M/946M [00:08<00:00, 122MB/s]\n",
            "Dataset unzipped to: /content/veri-vehicle-re-identification-dataset\n",
            "Kaggle dataset download and extraction complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Install the Kaggle API client\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Create a directory for Kaggle credentials if it doesn't exist\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Copy the kaggle.json file to the credentials directory\n",
        "# This assumes kaggle.json has been uploaded to /content/\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set permissions for the kaggle.json file\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Define dataset identifier and download path\n",
        "dataset_id = \"abhyudaya12/veri-vehicle-re-identification-dataset\"\n",
        "download_path = \"/content/veri-vehicle-re-identification-dataset\"\n",
        "\n",
        "# Create the download directory if it doesn't exist\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download {dataset_id} -p {download_path}\n",
        "\n",
        "# Unzip the downloaded dataset\n",
        "# Assuming the downloaded file is a zip archive in the download_path\n",
        "# We'll find the first .zip file and unzip it\n",
        "import zipfile\n",
        "\n",
        "for file_name in os.listdir(download_path):\n",
        "    if file_name.endswith('.zip'):\n",
        "        zip_file_path = os.path.join(download_path, file_name)\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(download_path)\n",
        "        print(f\"Dataset unzipped to: {download_path}\")\n",
        "        # Optionally, remove the zip file after extraction\n",
        "        # os.remove(zip_file_path)\n",
        "        break\n",
        "else:\n",
        "    print(\"No zip file found to extract.\")\n",
        "\n",
        "print(\"Kaggle dataset download and extraction complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVpYiVw4ZM31",
        "outputId": "85dd7a9d-1f2b-49b8-f9d3-0e63206db890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.234-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.234-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.234 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "VERI_DIR = \"/content/veri-vehicle-re-identification-dataset/VeRi\"\n",
        "OUTPUT_DIR = \"/content/veri_yolo_cls\"\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "print(\"ﾄ紳ng chuy盻ハ ﾄ黛ｻ品 VeRi dataset...\")\n",
        "\n",
        "image_dir = os.path.join(VERI_DIR, \"image_train\")\n",
        "\n",
        "# Xﾃｳa output cﾅｩ\n",
        "if os.path. exists(OUTPUT_DIR):\n",
        "    shutil.rmtree(OUTPUT_DIR)\n",
        "\n",
        "# T蘯｡o thﾆｰ m盻･c\n",
        "output_train = os.path.join(OUTPUT_DIR, \"train\")\n",
        "output_val = os.path.join(OUTPUT_DIR, \"val\")\n",
        "os.makedirs(output_train)\n",
        "os.makedirs(output_val)\n",
        "\n",
        "# Nhﾃｳm 蘯｣nh theo vehicle_id\n",
        "vehicle_images = defaultdict(list)\n",
        "jpg_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
        "\n",
        "for img_name in jpg_files:\n",
        "    vehicle_id = img_name.split('_')[0]\n",
        "    vehicle_images[vehicle_id].append(img_name)\n",
        "\n",
        "print(f\"{len(vehicle_images)} vehicles, {len(jpg_files)} 蘯｣nh\")\n",
        "\n",
        "# L盻皇 vﾃ chia train/val\n",
        "vehicle_images = {k: v for k, v in vehicle_images.items() if len(v) >= 2}\n",
        "total_train, total_val = 0, 0\n",
        "\n",
        "for vehicle_id, images in vehicle_images.items():\n",
        "    os.makedirs(os.path.join(output_train, vehicle_id), exist_ok=True)\n",
        "    os.makedirs(os.path. join(output_val, vehicle_id), exist_ok=True)\n",
        "\n",
        "    train_imgs, val_imgs = train_test_split(images, test_size=VAL_SPLIT, random_state=42)\n",
        "\n",
        "    for img in train_imgs:\n",
        "        shutil.copy2(os.path.join(image_dir, img), os.path.join(output_train, vehicle_id, img))\n",
        "        total_train += 1\n",
        "    for img in val_imgs:\n",
        "        shutil.copy2(os.path.join(image_dir, img), os.path. join(output_val, vehicle_id, img))\n",
        "        total_val += 1\n",
        "\n",
        "print(f\"Done! Train: {total_train}, Val: {total_val}, Classes: {len(vehicle_images)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l50VCqXnZC_z",
        "outputId": "44681155-a213-4b2c-8052-b5f012dfebe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "沐 ﾄ紳ng chuy盻ハ ﾄ黛ｻ品 VeRi dataset...\n",
            "沒 576 vehicles, 37778 蘯｣nh\n",
            "笨 Done! Train: 29981, Val: 7797, Classes: 576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "\n",
        "model = YOLO(\"yolo11n-cls.pt\")\n",
        "\n",
        "results = model.train(\n",
        "    data=\"/content/veri_yolo_cls\",\n",
        "    epochs=50,\n",
        "    imgsz=224,\n",
        "    batch=32,\n",
        "    device=\"cuda\",\n",
        "    patience=10,\n",
        "    exist_ok=True,\n",
        ")\n",
        "\n",
        "# Save model\n",
        "shutil.copy2(f\"{results.save_dir}/weights/best.pt\", \"/content/yolo11n-cls-veri.pt\")\n",
        "print(\"沁 Model saved: /content/yolo11n-cls-veri.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5ZtrCDEa_sf",
        "outputId": "4ee1c2f5-e5fa-4fe5-d5ed-a30ae5d8e49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file 笨 \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt to 'yolo11n-cls.pt': 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 5.5MB 57.5MB/s 0.1s\n",
            "Ultralytics 8.3.234 泅 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/veri_yolo_cls, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/classify/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/veri_yolo_cls/train... found 29981 images in 576 classes 笨 \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/veri_yolo_cls/val... found 7797 images in 576 classes 笨 \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "Overriding model.yaml nc=80 with nc=576\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1   1068096  ultralytics.nn.modules.head.Classify         [256, 576]                    \n",
            "YOLO11n-cls summary: 86 layers, 2,268,960 parameters, 2,268,960 gradients, 3.8 GFLOPs\n",
            "Transferred 234/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 5.4MB 86.9MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed 笨\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access 笨 (ping: 0.0ﾂｱ0.0 ms, read: 607.6ﾂｱ303.2 MB/s, size: 23.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/veri_yolo_cls/train... 29981 images, 0 corrupt: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 29981/29981 6.1Kit/s 4.9s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/veri_yolo_cls/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access 笨 (ping: 0.0ﾂｱ0.0 ms, read: 685.7ﾂｱ222.3 MB/s, size: 16.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/veri_yolo_cls/val... 7797 images, 0 corrupt: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 7797/7797 3.7Kit/s 2.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/veri_yolo_cls/val.cache\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/classify/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 755.1KB 20.2MB/s 0.0s\n",
            "\u001b[K       1/50     0.494G        5.6         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.6it/s 2:48\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.7it/s 21.5s\n",
            "                   all      0.195      0.399\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       2/50     0.604G      3.128         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:38\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.5s\n",
            "                   all      0.513      0.794\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       3/50     0.609G      2.062         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.7s\n",
            "                   all      0.494        0.8\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       4/50     0.615G      1.815         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.8it/s 2:42\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.8s\n",
            "                   all        0.7      0.919\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       5/50     0.621G      1.304         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.8s\n",
            "                   all      0.782      0.958\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       6/50     0.627G     0.9917         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.1it/s 19.9s\n",
            "                   all      0.805      0.965\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       7/50     0.633G     0.8045         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.874      0.984\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       8/50     0.639G     0.6511         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.6s\n",
            "                   all      0.899      0.988\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K       9/50     0.645G     0.5666         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.6s\n",
            "                   all      0.919      0.992\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      10/50      0.65G     0.5017         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:38\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.3it/s 19.5s\n",
            "                   all      0.932      0.994\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      11/50     0.656G      0.442         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:38\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.5s\n",
            "                   all      0.938      0.995\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      12/50     0.662G     0.4065         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.949      0.994\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      13/50     0.668G     0.3754         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:36\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.958      0.996\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      14/50     0.674G     0.3409         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:36\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.1it/s 19.9s\n",
            "                   all      0.964      0.998\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      15/50      0.68G     0.3244         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.7s\n",
            "                   all      0.968      0.998\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      16/50     0.686G     0.3059         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.5s\n",
            "                   all      0.973      0.998\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      17/50     0.691G     0.2846         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.8it/s 2:41\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.976      0.998\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      18/50     0.697G     0.2654         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:38\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.977      0.998\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      19/50     0.703G     0.2444         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.8s\n",
            "                   all      0.981      0.998\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      20/50     0.709G      0.236         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:40\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.6s\n",
            "                   all      0.984      0.998\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      21/50     0.715G     0.2176         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.7s\n",
            "                   all      0.984      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      22/50     0.721G     0.2108         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:36\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.986      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      23/50     0.727G     0.1925         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.8s\n",
            "                   all      0.986      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      24/50     0.732G     0.1872         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.6s\n",
            "                   all      0.987      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      25/50     0.738G     0.1672         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.0it/s 20.2s\n",
            "                   all      0.987      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      26/50     0.744G     0.1636         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:38\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.7it/s 21.3s\n",
            "                   all      0.987      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      27/50      0.75G     0.1633         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:38\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 20.9s\n",
            "                   all      0.988      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      28/50     0.756G     0.1524         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.2s\n",
            "                   all      0.988      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      29/50     0.762G      0.141         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:38\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.8s\n",
            "                   all      0.989      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      30/50     0.768G     0.1328         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.0it/s 20.2s\n",
            "                   all      0.989      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      31/50     0.773G     0.1285         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.1it/s 20.0s\n",
            "                   all      0.989      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      32/50     0.779G     0.1224         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.8s\n",
            "                   all      0.989      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      33/50     0.783G     0.1166         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.6s\n",
            "                   all      0.989      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      34/50     0.789G     0.1102         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.0it/s 20.4s\n",
            "                   all       0.99      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      35/50     0.795G     0.1036         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:37\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.7s\n",
            "                   all       0.99      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      36/50     0.801G    0.09456         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:36\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.3it/s 19.3s\n",
            "                   all       0.99      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      37/50     0.807G    0.09042         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:36\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.6s\n",
            "                   all       0.99      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      38/50     0.812G    0.08689         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 6.0it/s 2:35\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.991      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      39/50     0.818G    0.07557         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.7it/s 2:45\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 20.9s\n",
            "                   all      0.991      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      40/50     0.824G    0.07386         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.7it/s 2:44\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.1it/s 20.0s\n",
            "                   all      0.991      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      41/50      0.83G    0.07107         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.7it/s 2:44\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 20.9s\n",
            "                   all      0.991      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      42/50     0.836G    0.06623         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.9it/s 2:39\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.2it/s 19.6s\n",
            "                   all      0.991          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      43/50     0.842G    0.05913         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.8it/s 2:40\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.0it/s 20.3s\n",
            "                   all      0.991          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      44/50     0.848G    0.05568         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.8it/s 2:41\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.1it/s 20.0s\n",
            "                   all      0.991          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      45/50     0.854G    0.05399         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.8it/s 2:41\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 6.1it/s 20.0s\n",
            "                   all      0.991          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      46/50     0.859G    0.04858         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.7it/s 2:45\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.9it/s 20.6s\n",
            "                   all      0.991          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      47/50     0.865G    0.04628         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.8it/s 2:42\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.991          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      48/50     0.871G    0.04571         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.7it/s 2:44\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.991          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      49/50     0.877G    0.04096         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.8it/s 2:42\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.991          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K      50/50     0.883G    0.04021         29        224: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 937/937 5.7it/s 2:44\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 21.0s\n",
            "                   all      0.991          1\n",
            "\n",
            "50 epochs completed in 2.501 hours.\n",
            "Optimizer stripped from /content/runs/classify/train/weights/last.pt, 4.7MB\n",
            "Optimizer stripped from /content/runs/classify/train/weights/best.pt, 4.7MB\n",
            "\n",
            "Validating /content/runs/classify/train/weights/best.pt...\n",
            "Ultralytics 8.3.234 泅 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 2,263,880 parameters, 0 gradients, 3.8 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/veri_yolo_cls/train... found 29981 images in 576 classes 笨 \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/veri_yolo_cls/val... found 7797 images in 576 classes 笨 \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% 笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤 122/122 5.8it/s 20.9s\n",
            "                   all      0.991          1\n",
            "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/classify/train\u001b[0m\n",
            "沁 Model saved: /content/yolo11n-cls-veri.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ReID evaluate\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================\n",
        "# CONFIG\n",
        "# ==========================\n",
        "VERI_DIR = \"/content/veri-vehicle-re-identification-dataset/VeRi\"\n",
        "MODEL_PATH = \"/content/runs/classify/train/weights/best.pt\"\n",
        "DEVICE = \"cuda\" if torch. cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ==========================\n",
        "# FUNCTIONS\n",
        "# ==========================\n",
        "def parse_filename(filename):\n",
        "    parts = filename.split('_')\n",
        "    vehicle_id = int(parts[0])\n",
        "    camera_id = int(parts[1][1:])\n",
        "    return vehicle_id, camera_id\n",
        "\n",
        "def extract_features(model, image_dir):\n",
        "    transform = transforms.Compose([\n",
        "        transforms. Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms. Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    features, labels, cam_ids = [], [], []\n",
        "    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    print(f\"   沒ｷ Processing {len(image_files)} images...\")\n",
        "\n",
        "    batch_size = 32\n",
        "    for i in tqdm(range(0, len(image_files), batch_size), desc=\"   Extracting\"):\n",
        "        batch_files = image_files[i:i+batch_size]\n",
        "        batch_images, batch_labels, batch_cams = [], [], []\n",
        "\n",
        "        for img_name in batch_files:\n",
        "            try:\n",
        "                vid, cid = parse_filename(img_name)\n",
        "                img = Image.open(os.path.join(image_dir, img_name)). convert('RGB')\n",
        "                batch_images.append(transform(img))\n",
        "                batch_labels.append(vid)\n",
        "                batch_cams.append(cid)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if len(batch_images) == 0:\n",
        "            continue\n",
        "\n",
        "        labels.extend(batch_labels)\n",
        "        cam_ids. extend(batch_cams)\n",
        "\n",
        "        batch_tensor = torch.stack(batch_images). to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            results = model(batch_tensor, verbose=False)\n",
        "            for result in results:\n",
        "                features.append(result. probs. data.cpu().numpy())\n",
        "\n",
        "    return np.array(features), np. array(labels), np.array(cam_ids)\n",
        "\n",
        "def compute_metrics(q_feat, q_labels, q_cams, g_feat, g_labels, g_cams):\n",
        "    # Normalize\n",
        "    q_feat = q_feat / (np.linalg.norm(q_feat, axis=1, keepdims=True) + 1e-8)\n",
        "    g_feat = g_feat / (np.linalg.norm(g_feat, axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "    # Distance matrix\n",
        "    dist = 1 - np. dot(q_feat, g_feat. T)\n",
        "\n",
        "    all_AP, all_cmc = [], np.zeros(50)\n",
        "    num_valid = 0\n",
        "\n",
        "    for i in range(len(q_labels)):\n",
        "        q_label, q_cam = q_labels[i], q_cams[i]\n",
        "\n",
        "        order = np.argsort(dist[i])\n",
        "        sorted_labels = g_labels[order]\n",
        "        sorted_cams = g_cams[order]\n",
        "\n",
        "        # Remove junk\n",
        "        keep = ~((sorted_labels == q_label) & (sorted_cams == q_cam))\n",
        "        matches = (sorted_labels[keep] == q_label). astype(float)\n",
        "\n",
        "        if np.sum(matches) == 0:\n",
        "            continue\n",
        "\n",
        "        num_valid += 1\n",
        "\n",
        "        # CMC\n",
        "        first_match = np.where(matches == 1)[0]\n",
        "        if len(first_match) > 0 and first_match[0] < 50:\n",
        "            all_cmc[first_match[0]:] += 1\n",
        "\n",
        "        # AP\n",
        "        cumsum = np.cumsum(matches)\n",
        "        precision = cumsum / (np.arange(len(matches)) + 1)\n",
        "        AP = np.sum(precision * matches) / np.sum(matches)\n",
        "        all_AP.append(AP)\n",
        "\n",
        "    mAP = np. mean(all_AP) * 100\n",
        "    cmc = all_cmc / num_valid * 100\n",
        "\n",
        "    return mAP, cmc\n",
        "\n",
        "# ==========================\n",
        "# MAIN\n",
        "# ==========================\n",
        "print(\"=\"*60)\n",
        "print(\" ﾄ静¨H GIﾃ REID MODEL TRﾃ劾 VERI DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "query_dir = os.path.join(VERI_DIR, \"image_query\")\n",
        "gallery_dir = os. path.join(VERI_DIR, \"image_test\")\n",
        "\n",
        "print(f\"\\n Model: {MODEL_PATH}\")\n",
        "model = YOLO(MODEL_PATH)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Extract features\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\" QUERY SET:\")\n",
        "q_feat, q_labels, q_cams = extract_features(model, query_dir)\n",
        "print(f\"    {len(q_labels)} images, {len(set(q_labels))} vehicles\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\" GALLERY SET:\")\n",
        "g_feat, g_labels, g_cams = extract_features(model, gallery_dir)\n",
        "print(f\"    {len(g_labels)} images, {len(set(g_labels))} vehicles\")\n",
        "\n",
        "# Compute metrics\n",
        "print(\"\\n\" + \"-\"*40)\n",
        "print(\" Computing metrics...\")\n",
        "mAP, cmc = compute_metrics(q_feat, q_labels, q_cams, g_feat, g_labels, g_cams)\n",
        "\n",
        "# Results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" K蘯ｾT QU蘯｢ REID METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"   mAP:      {mAP:.2f}%\")\n",
        "print(f\"   Rank-1:   {cmc[0]:.2f}%\")\n",
        "print(f\"   Rank-5:   {cmc[4]:.2f}%\")\n",
        "print(f\"   Rank-10:  {cmc[9]:.2f}%\")\n",
        "print(f\"   Rank-20:  {cmc[19]:.2f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Plot CMC\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt. plot(range(1, 51), cmc, 'b-', linewidth=2)\n",
        "plt.xlabel('Rank', fontsize=12)\n",
        "plt.ylabel('Matching Rate (%)', fontsize=12)\n",
        "plt. title('CMC Curve - YOLOv11n-cls on VeRi Dataset', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim([1, 50])\n",
        "plt.ylim([0, 100])\n",
        "plt.scatter([1, 5, 10], [cmc[0], cmc[4], cmc[9]], color=['red', 'green', 'orange'], s=100, zorder=5)\n",
        "plt.legend([f'Rank-1: {cmc[0]:.1f}%', f'Rank-5: {cmc[4]:.1f}%', f'Rank-10: {cmc[9]:.1f}%'], loc='lower right')\n",
        "plt. tight_layout()\n",
        "plt.savefig('/content/cmc_curve.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nCMC curve saved: /content/cmc_curve.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MNjbjgPvF1DT",
        "outputId": "836f5c85-fc85-4b0f-8762-8904c7c65881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "沐 ﾄ静¨H GIﾃ REID MODEL TRﾃ劾 VERI DATASET\n",
            "============================================================\n",
            "\n",
            "沒ｦ Model: /content/runs/classify/train/weights/best.pt\n",
            "\n",
            "----------------------------------------\n",
            "沒 QUERY SET:\n",
            "   沒ｷ Processing 1678 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   0%|          | 0/53 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   2%|笆         | 1/53 [00:00<00:06,  7.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   6%|笆         | 3/53 [00:00<00:05,  9.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   8%|笆         | 4/53 [00:00<00:05,  9.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   9%|笆         | 5/53 [00:00<00:05,  9.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  11%|笆遺柾        | 6/53 [00:00<00:04,  9.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  13%|笆遺鮪        | 7/53 [00:00<00:04,  9.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  15%|笆遺膜        | 8/53 [00:00<00:04,  9.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  17%|笆遺幕        | 9/53 [00:00<00:05,  8.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  19%|笆遺哩        | 10/53 [00:01<00:05,  8.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  21%|笆遺毎        | 11/53 [00:01<00:05,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5877127647399902. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  23%|笆遺毎笆       | 12/53 [00:01<00:04,  8.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  25%|笆遺毎笆       | 13/53 [00:01<00:04,  8.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  26%|笆遺毎笆       | 14/53 [00:01<00:04,  8.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  28%|笆遺毎笆       | 15/53 [00:01<00:04,  9.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  30%|笆遺毎笆       | 16/53 [00:01<00:04,  9.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  32%|笆遺毎笆遺柾      | 17/53 [00:01<00:03,  9.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  34%|笆遺毎笆遺枕      | 18/53 [00:01<00:03,  9.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  36%|笆遺毎笆遺膜      | 19/53 [00:02<00:03,  8.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  38%|笆遺毎笆遺槙      | 20/53 [00:02<00:03,  8.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  40%|笆遺毎笆遺哩      | 21/53 [00:02<00:03,  8.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  43%|笆遺毎笆遺毎笆     | 23/53 [00:02<00:03,  9.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  45%|笆遺毎笆遺毎笆     | 24/53 [00:02<00:03,  9.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  47%|笆遺毎笆遺毎笆     | 25/53 [00:02<00:02,  9.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  49%|笆遺毎笆遺毎笆     | 26/53 [00:02<00:02,  9.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  51%|笆遺毎笆遺毎笆     | 27/53 [00:02<00:02,  9.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  53%|笆遺毎笆遺毎笆遺鮪    | 28/53 [00:03<00:03,  8.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  55%|笆遺毎笆遺毎笆遺枕    | 29/53 [00:03<00:03,  7.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  57%|笆遺毎笆遺毎笆遺幕    | 30/53 [00:03<00:02,  8.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  58%|笆遺毎笆遺毎笆遺槙    | 31/53 [00:03<00:02,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  62%|笆遺毎笆遺毎笆遺毎笆   | 33/53 [00:03<00:02,  9.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  66%|笆遺毎笆遺毎笆遺毎笆   | 35/53 [00:03<00:01,  9.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  68%|笆遺毎笆遺毎笆遺毎笆   | 36/53 [00:04<00:01,  9.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  72%|笆遺毎笆遺毎笆遺毎笆遺柾  | 38/53 [00:04<00:01,  9.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  74%|笆遺毎笆遺毎笆遺毎笆遺鮪  | 39/53 [00:04<00:01,  9.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  75%|笆遺毎笆遺毎笆遺毎笆遺膜  | 40/53 [00:04<00:01,  9.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  77%|笆遺毎笆遺毎笆遺毎笆遺幕  | 41/53 [00:04<00:01,  9.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  79%|笆遺毎笆遺毎笆遺毎笆遺哩  | 42/53 [00:04<00:01,  9.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  83%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 44/53 [00:04<00:00,  9.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  85%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 45/53 [00:04<00:00,  9.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  87%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 46/53 [00:05<00:00,  9.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  89%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 47/53 [00:05<00:00,  9.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  91%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 48/53 [00:05<00:00,  9.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  94%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺枕| 50/53 [00:05<00:00,  9.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  98%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺槙| 52/53 [00:05<00:00, 10.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 53/53 [00:05<00:00,  9.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   笨 1678 images, 200 vehicles\n",
            "\n",
            "----------------------------------------\n",
            "沒 GALLERY SET:\n",
            "   沒ｷ Processing 11579 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   0%|          | 0/362 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.517995834350586. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   1%|          | 2/362 [00:00<00:30, 11.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5702831745147705. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5877127647399902. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   1%|          | 4/362 [00:00<00:32, 11.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6051416397094727. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   2%|笆         | 6/362 [00:00<00:32, 10.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   2%|笆         | 8/362 [00:00<00:31, 11.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   3%|笆         | 10/362 [00:00<00:31, 11.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   3%|笆         | 12/362 [00:01<00:30, 11.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   4%|笆         | 14/362 [00:01<00:31, 10.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   4%|笆         | 16/362 [00:01<00:31, 10.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   5%|笆         | 18/362 [00:01<00:30, 11.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   6%|笆         | 20/362 [00:01<00:33, 10.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   6%|笆         | 22/362 [00:02<00:35,  9.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   6%|笆         | 23/362 [00:02<00:35,  9.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   7%|笆         | 25/362 [00:02<00:34,  9.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   7%|笆         | 27/362 [00:02<00:33, 10.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   8%|笆         | 29/362 [00:02<00:32, 10.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   9%|笆         | 31/362 [00:02<00:31, 10.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5877127647399902. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:   9%|笆         | 33/362 [00:03<00:30, 10.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  10%|笆         | 35/362 [00:03<00:34,  9.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.552854299545288. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  10%|笆         | 36/362 [00:03<00:35,  9.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  10%|笆         | 37/362 [00:03<00:37,  8.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  10%|笆         | 38/362 [00:03<00:38,  8.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  11%|笆         | 39/362 [00:03<00:38,  8.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  11%|笆         | 40/362 [00:04<00:39,  8.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  11%|笆遺柾        | 41/362 [00:04<00:40,  7.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  12%|笆遺柾        | 42/362 [00:04<00:39,  8.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  12%|笆遺柾        | 43/362 [00:04<00:40,  7.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  12%|笆遺柾        | 44/362 [00:04<00:41,  7.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  12%|笆遺柾        | 45/362 [00:04<00:40,  7.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  13%|笆遺鮪        | 46/362 [00:04<00:40,  7.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  13%|笆遺鮪        | 47/362 [00:04<00:44,  7.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  13%|笆遺鮪        | 48/362 [00:05<00:46,  6.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  14%|笆遺鮪        | 49/362 [00:05<00:47,  6.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  14%|笆遺枕        | 50/362 [00:05<00:47,  6.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  14%|笆遺枕        | 51/362 [00:05<00:48,  6.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  14%|笆遺枕        | 52/362 [00:05<00:45,  6.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  15%|笆遺枕        | 54/362 [00:05<00:39,  7.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  15%|笆遺膜        | 55/362 [00:06<00:38,  8.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  15%|笆遺膜        | 56/362 [00:06<00:37,  8.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  16%|笆遺膜        | 57/362 [00:06<00:36,  8.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  16%|笆遺膜        | 58/362 [00:06<00:35,  8.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6051416397094727. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  16%|笆遺幕        | 59/362 [00:06<00:34,  8.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  17%|笆遺幕        | 60/362 [00:06<00:36,  8.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  17%|笆遺幕        | 61/362 [00:06<00:36,  8.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  17%|笆遺幕        | 62/362 [00:06<00:35,  8.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  17%|笆遺幕        | 63/362 [00:07<00:35,  8.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  18%|笆遺槙        | 64/362 [00:07<00:34,  8.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  18%|笆遺槙        | 65/362 [00:07<00:33,  8.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  19%|笆遺槙        | 67/362 [00:07<00:32,  9.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  19%|笆遺哩        | 68/362 [00:07<00:31,  9.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  19%|笆遺哩        | 69/362 [00:07<00:33,  8.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6051416397094727. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  19%|笆遺哩        | 70/362 [00:07<00:32,  8.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  20%|笆遺哩        | 71/362 [00:07<00:32,  9.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  20%|笆遺毎        | 73/362 [00:08<00:29,  9.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  20%|笆遺毎        | 74/362 [00:08<00:29,  9.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  21%|笆遺毎        | 75/362 [00:08<00:29,  9.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.483137607574463. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  21%|笆遺毎笆       | 77/362 [00:08<00:31,  9.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  22%|笆遺毎笆       | 78/362 [00:08<00:30,  9.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  22%|笆遺毎笆       | 80/362 [00:08<00:29,  9.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6051416397094727. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  23%|笆遺毎笆       | 82/362 [00:09<00:28, 10.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  23%|笆遺毎笆       | 84/362 [00:09<00:26, 10.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  24%|笆遺毎笆       | 86/362 [00:09<00:25, 10.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  24%|笆遺毎笆       | 88/362 [00:09<00:25, 10.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6051416397094727. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  25%|笆遺毎笆       | 90/362 [00:09<00:25, 10.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6051416397094727. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  25%|笆遺毎笆       | 92/362 [00:09<00:25, 10.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  26%|笆遺毎笆       | 94/362 [00:10<00:24, 11.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  27%|笆遺毎笆       | 96/362 [00:10<00:24, 11.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  27%|笆遺毎笆       | 98/362 [00:10<00:23, 11.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  28%|笆遺毎笆       | 100/362 [00:10<00:23, 10.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  28%|笆遺毎笆       | 102/362 [00:10<00:23, 10.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  29%|笆遺毎笆       | 104/362 [00:10<00:23, 10.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  29%|笆遺毎笆       | 106/362 [00:11<00:23, 10.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  30%|笆遺毎笆       | 108/362 [00:11<00:22, 11.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  30%|笆遺毎笆       | 110/362 [00:11<00:23, 10.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  31%|笆遺毎笆       | 112/362 [00:11<00:23, 10.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  31%|笆遺毎笆遺柾      | 114/362 [00:11<00:23, 10.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  32%|笆遺毎笆遺柾      | 116/362 [00:12<00:22, 10.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  33%|笆遺毎笆遺鮪      | 118/362 [00:12<00:22, 10.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  33%|笆遺毎笆遺鮪      | 120/362 [00:12<00:23, 10.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  34%|笆遺毎笆遺鮪      | 122/362 [00:12<00:24,  9.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  34%|笆遺毎笆遺枕      | 123/362 [00:12<00:25,  9.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  34%|笆遺毎笆遺枕      | 124/362 [00:12<00:26,  8.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  35%|笆遺毎笆遺枕      | 125/362 [00:13<00:28,  8.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  35%|笆遺毎笆遺枕      | 126/362 [00:13<00:27,  8.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  35%|笆遺毎笆遺膜      | 128/362 [00:13<00:25,  9.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  36%|笆遺毎笆遺膜      | 130/362 [00:13<00:23,  9.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  36%|笆遺毎笆遺幕      | 132/362 [00:13<00:23,  9.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  37%|笆遺毎笆遺幕      | 133/362 [00:13<00:24,  9.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  37%|笆遺毎笆遺幕      | 134/362 [00:14<00:23,  9.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  38%|笆遺毎笆遺槙      | 136/362 [00:14<00:22, 10.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  38%|笆遺毎笆遺槙      | 138/362 [00:14<00:21, 10.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  39%|笆遺毎笆遺槙      | 140/362 [00:14<00:20, 10.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  39%|笆遺毎笆遺哩      | 142/362 [00:14<00:20, 10.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  40%|笆遺毎笆遺哩      | 144/362 [00:14<00:20, 10.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  40%|笆遺毎笆遺毎      | 146/362 [00:15<00:19, 10.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  41%|笆遺毎笆遺毎      | 148/362 [00:15<00:19, 10.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  41%|笆遺毎笆遺毎笆     | 150/362 [00:15<00:19, 10.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  42%|笆遺毎笆遺毎笆     | 152/362 [00:15<00:20, 10.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  43%|笆遺毎笆遺毎笆     | 154/362 [00:15<00:22,  9.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  43%|笆遺毎笆遺毎笆     | 155/362 [00:16<00:22,  9.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  43%|笆遺毎笆遺毎笆     | 156/362 [00:16<00:23,  8.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  43%|笆遺毎笆遺毎笆     | 157/362 [00:16<00:23,  8.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  44%|笆遺毎笆遺毎笆     | 158/362 [00:16<00:23,  8.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  44%|笆遺毎笆遺毎笆     | 159/362 [00:16<00:23,  8.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  44%|笆遺毎笆遺毎笆     | 160/362 [00:16<00:23,  8.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  44%|笆遺毎笆遺毎笆     | 161/362 [00:16<00:24,  8.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  45%|笆遺毎笆遺毎笆     | 162/362 [00:16<00:24,  8.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  45%|笆遺毎笆遺毎笆     | 163/362 [00:17<00:24,  7.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  45%|笆遺毎笆遺毎笆     | 164/362 [00:17<00:24,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  46%|笆遺毎笆遺毎笆     | 165/362 [00:17<00:25,  7.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  46%|笆遺毎笆遺毎笆     | 166/362 [00:17<00:25,  7.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  46%|笆遺毎笆遺毎笆     | 167/362 [00:17<00:25,  7.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  46%|笆遺毎笆遺毎笆     | 168/362 [00:17<00:25,  7.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  47%|笆遺毎笆遺毎笆     | 169/362 [00:17<00:26,  7.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  47%|笆遺毎笆遺毎笆     | 170/362 [00:18<00:26,  7.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  48%|笆遺毎笆遺毎笆     | 172/362 [00:18<00:22,  8.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  48%|笆遺毎笆遺毎笆     | 174/362 [00:18<00:20,  9.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  49%|笆遺毎笆遺毎笆     | 176/362 [00:18<00:18,  9.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  49%|笆遺毎笆遺毎笆     | 178/362 [00:18<00:18, 10.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  50%|笆遺毎笆遺毎笆     | 180/362 [00:18<00:17, 10.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  50%|笆遺毎笆遺毎笆     | 182/362 [00:19<00:18,  9.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.552854299545288. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  51%|笆遺毎笆遺毎笆     | 183/362 [00:19<00:19,  9.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  51%|笆遺毎笆遺毎笆     | 184/362 [00:19<00:20,  8.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5354249477386475. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  51%|笆遺毎笆遺毎笆     | 185/362 [00:19<00:20,  8.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  51%|笆遺毎笆遺毎笆遺柾    | 186/362 [00:19<00:21,  8.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.517995834350586. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  52%|笆遺毎笆遺毎笆遺柾    | 187/362 [00:19<00:21,  8.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.517995834350586. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  52%|笆遺毎笆遺毎笆遺柾    | 188/362 [00:19<00:21,  8.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.552854299545288. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  52%|笆遺毎笆遺毎笆遺柾    | 189/362 [00:20<00:21,  7.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  52%|笆遺毎笆遺毎笆遺柾    | 190/362 [00:20<00:21,  7.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5877127647399902. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  53%|笆遺毎笆遺毎笆遺鮪    | 191/362 [00:20<00:21,  7.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.3785624504089355. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  53%|笆遺毎笆遺毎笆遺鮪    | 192/362 [00:20<00:21,  7.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5702831745147705. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  53%|笆遺毎笆遺毎笆遺鮪    | 193/362 [00:20<00:21,  8.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5877127647399902. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  54%|笆遺毎笆遺毎笆遺鮪    | 194/362 [00:20<00:21,  7.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5354249477386475. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  54%|笆遺毎笆遺毎笆遺枕    | 195/362 [00:20<00:22,  7.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5005664825439453. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  54%|笆遺毎笆遺毎笆遺枕    | 196/362 [00:21<00:21,  7.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5877127647399902. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  54%|笆遺毎笆遺毎笆遺枕    | 197/362 [00:21<00:21,  7.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  55%|笆遺毎笆遺毎笆遺枕    | 198/362 [00:21<00:20,  7.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  55%|笆遺毎笆遺毎笆遺枕    | 199/362 [00:21<00:21,  7.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  55%|笆遺毎笆遺毎笆遺膜    | 200/362 [00:21<00:19,  8.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  56%|笆遺毎笆遺毎笆遺膜    | 202/362 [00:21<00:17,  9.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  56%|笆遺毎笆遺毎笆遺幕    | 204/362 [00:21<00:16,  9.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  57%|笆遺毎笆遺毎笆遺幕    | 206/362 [00:22<00:15, 10.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  57%|笆遺毎笆遺毎笆遺幕    | 208/362 [00:22<00:15, 10.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  58%|笆遺毎笆遺毎笆遺槙    | 210/362 [00:22<00:15,  9.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  59%|笆遺毎笆遺毎笆遺槙    | 212/362 [00:22<00:14, 10.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  59%|笆遺毎笆遺毎笆遺哩    | 214/362 [00:22<00:14, 10.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  60%|笆遺毎笆遺毎笆遺哩    | 216/362 [00:23<00:13, 10.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  60%|笆遺毎笆遺毎笆遺毎    | 218/362 [00:23<00:13, 10.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  61%|笆遺毎笆遺毎笆遺毎    | 220/362 [00:23<00:13, 10.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  61%|笆遺毎笆遺毎笆遺毎笆   | 222/362 [00:23<00:12, 10.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  62%|笆遺毎笆遺毎笆遺毎笆   | 224/362 [00:23<00:12, 10.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  62%|笆遺毎笆遺毎笆遺毎笆   | 226/362 [00:23<00:12, 11.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  63%|笆遺毎笆遺毎笆遺毎笆   | 228/362 [00:24<00:11, 11.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  64%|笆遺毎笆遺毎笆遺毎笆   | 230/362 [00:24<00:12, 10.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  64%|笆遺毎笆遺毎笆遺毎笆   | 232/362 [00:24<00:11, 11.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  65%|笆遺毎笆遺毎笆遺毎笆   | 234/362 [00:24<00:11, 11.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  65%|笆遺毎笆遺毎笆遺毎笆   | 236/362 [00:24<00:11, 11.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  66%|笆遺毎笆遺毎笆遺毎笆   | 238/362 [00:25<00:11, 11.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  66%|笆遺毎笆遺毎笆遺毎笆   | 240/362 [00:25<00:11, 11.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  67%|笆遺毎笆遺毎笆遺毎笆   | 242/362 [00:25<00:10, 11.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  67%|笆遺毎笆遺毎笆遺毎笆   | 244/362 [00:25<00:10, 11.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  68%|笆遺毎笆遺毎笆遺毎笆   | 246/362 [00:25<00:10, 11.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  69%|笆遺毎笆遺毎笆遺毎笆   | 248/362 [00:25<00:10, 11.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  69%|笆遺毎笆遺毎笆遺毎笆   | 250/362 [00:26<00:10, 11.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  70%|笆遺毎笆遺毎笆遺毎笆   | 252/362 [00:26<00:09, 11.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  70%|笆遺毎笆遺毎笆遺毎笆   | 254/362 [00:26<00:09, 11.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  71%|笆遺毎笆遺毎笆遺毎笆   | 256/362 [00:26<00:09, 11.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  71%|笆遺毎笆遺毎笆遺毎笆遺柾  | 258/362 [00:26<00:09, 11.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  72%|笆遺毎笆遺毎笆遺毎笆遺柾  | 260/362 [00:26<00:09, 11.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  72%|笆遺毎笆遺毎笆遺毎笆遺柾  | 262/362 [00:27<00:08, 11.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  73%|笆遺毎笆遺毎笆遺毎笆遺鮪  | 264/362 [00:27<00:08, 10.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  73%|笆遺毎笆遺毎笆遺毎笆遺鮪  | 266/362 [00:27<00:08, 10.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  74%|笆遺毎笆遺毎笆遺毎笆遺枕  | 268/362 [00:27<00:08, 10.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5877127647399902. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  75%|笆遺毎笆遺毎笆遺毎笆遺枕  | 270/362 [00:27<00:08, 11.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  75%|笆遺毎笆遺毎笆遺毎笆遺膜  | 272/362 [00:28<00:08, 11.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  76%|笆遺毎笆遺毎笆遺毎笆遺膜  | 274/362 [00:28<00:08,  9.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  76%|笆遺毎笆遺毎笆遺毎笆遺膜  | 276/362 [00:28<00:09,  9.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  77%|笆遺毎笆遺毎笆遺毎笆遺幕  | 277/362 [00:28<00:09,  8.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  77%|笆遺毎笆遺毎笆遺毎笆遺幕  | 278/362 [00:28<00:09,  8.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  77%|笆遺毎笆遺毎笆遺毎笆遺幕  | 279/362 [00:28<00:10,  8.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  77%|笆遺毎笆遺毎笆遺毎笆遺幕  | 280/362 [00:29<00:10,  8.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  78%|笆遺毎笆遺毎笆遺毎笆遺槙  | 281/362 [00:29<00:09,  8.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  78%|笆遺毎笆遺毎笆遺毎笆遺槙  | 282/362 [00:29<00:10,  7.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  78%|笆遺毎笆遺毎笆遺毎笆遺槙  | 283/362 [00:29<00:09,  8.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  78%|笆遺毎笆遺毎笆遺毎笆遺槙  | 284/362 [00:29<00:10,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  79%|笆遺毎笆遺毎笆遺毎笆遺槙  | 285/362 [00:29<00:10,  7.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  79%|笆遺毎笆遺毎笆遺毎笆遺哩  | 286/362 [00:29<00:09,  7.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  79%|笆遺毎笆遺毎笆遺毎笆遺哩  | 287/362 [00:30<00:09,  7.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  80%|笆遺毎笆遺毎笆遺毎笆遺哩  | 288/362 [00:30<00:09,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  80%|笆遺毎笆遺毎笆遺毎笆遺哩  | 289/362 [00:30<00:09,  7.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  80%|笆遺毎笆遺毎笆遺毎笆遺毎  | 290/362 [00:30<00:09,  7.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  80%|笆遺毎笆遺毎笆遺毎笆遺毎  | 291/362 [00:30<00:09,  7.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  81%|笆遺毎笆遺毎笆遺毎笆遺毎  | 292/362 [00:30<00:08,  8.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  81%|笆遺毎笆遺毎笆遺毎笆遺毎  | 294/362 [00:30<00:07,  9.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  82%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 296/362 [00:31<00:06,  9.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  82%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 298/362 [00:31<00:06, 10.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  83%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 300/362 [00:31<00:05, 10.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  83%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 302/362 [00:31<00:05, 10.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  84%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 304/362 [00:31<00:05, 10.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  85%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 306/362 [00:31<00:05, 10.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  85%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 308/362 [00:32<00:04, 10.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  86%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 310/362 [00:32<00:04, 10.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  86%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 312/362 [00:32<00:04, 10.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  87%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 314/362 [00:32<00:04, 10.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  87%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 316/362 [00:32<00:04, 11.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  88%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 318/362 [00:33<00:04, 10.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  88%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 320/362 [00:33<00:03, 11.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  89%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 322/362 [00:33<00:03, 11.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  90%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 324/362 [00:33<00:03, 11.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  90%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 326/362 [00:33<00:03, 10.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  91%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 328/362 [00:33<00:03, 10.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  91%|笆遺毎笆遺毎笆遺毎笆遺毎笆 | 330/362 [00:34<00:02, 11.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  92%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺柾| 332/362 [00:34<00:02, 11.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  92%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺柾| 334/362 [00:34<00:02, 11.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.5877127647399902. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  93%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺鮪| 336/362 [00:34<00:02, 11.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  93%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺鮪| 338/362 [00:34<00:02, 11.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  94%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺枕| 340/362 [00:35<00:02, 10.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  94%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺枕| 342/362 [00:35<00:01, 11.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.6225709915161133. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  95%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺膜| 344/362 [00:35<00:01, 11.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  96%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺膜| 346/362 [00:35<00:01, 11.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  96%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺膜| 348/362 [00:35<00:01, 11.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  97%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺幕| 350/362 [00:35<00:01, 11.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  97%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺幕| 352/362 [00:36<00:00, 11.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  98%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺槙| 354/362 [00:36<00:00, 11.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  98%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺槙| 356/362 [00:36<00:00, 11.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  99%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺哩| 358/362 [00:36<00:00, 11.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r   Extracting:  99%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺哩| 360/362 [00:36<00:00, 11.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n",
            "WARNING 笞ｸ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 2.640000104904175. Dividing input by 255.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "   Extracting: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 362/362 [00:36<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   笨 11579 images, 200 vehicles\n",
            "\n",
            "----------------------------------------\n",
            "洫ｮ Computing metrics...\n",
            "\n",
            "============================================================\n",
            "沒 K蘯ｾT QU蘯｢ REID METRICS\n",
            "============================================================\n",
            "   mAP:      1.33%\n",
            "   Rank-1:   3.87%\n",
            "   Rank-5:   10.07%\n",
            "   Rank-10:  15.32%\n",
            "   Rank-20:  22.53%\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "沒 CMC curve saved: /content/cmc_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "query_dir = \"/content/veri-vehicle-re-identification-dataset/VeRi/image_test\"\n",
        "\n",
        "print(f\"   Ki盻ノ tra: {query_dir}\")\n",
        "print(f\"   T盻渡 t蘯｡i: {os.path. exists(query_dir)}\")\n",
        "\n",
        "# Li盻t kﾃｪ t蘯･t c蘯｣ files\n",
        "all_files = os. listdir(query_dir)\n",
        "print(f\"   T盻貧g files: {len(all_files)}\")\n",
        "\n",
        "# Hi盻n 10 file ﾄ黛ｺｧu\n",
        "print(f\"   10 file ﾄ黛ｺｧu tiﾃｪn:\")\n",
        "for f in all_files[:10]:\n",
        "    print(f\"      '{f}'\")\n",
        "\n",
        "# Ki盻ノ tra extension\n",
        "jpg_files = [f for f in all_files if f.endswith('.jpg')]\n",
        "JPG_files = [f for f in all_files if f. endswith('.JPG')]\n",
        "jpeg_files = [f for f in all_files if f.endswith('.jpeg')]\n",
        "\n",
        "print(f\"\\n   . jpg files: {len(jpg_files)}\")\n",
        "print(f\"   .JPG files: {len(JPG_files)}\")\n",
        "print(f\"   .jpeg files: {len(jpeg_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNWv6J38aScc",
        "outputId": "86194fd7-3a65-438e-f2e7-0db1a35686fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "沒 Ki盻ノ tra: /content/veri-vehicle-re-identification-dataset/VeRi/image_test\n",
            "   T盻渡 t蘯｡i: True\n",
            "   T盻貧g files: 11579\n",
            "   10 file ﾄ黛ｺｧu tiﾃｪn:\n",
            "      '0318_c012_00024130_0.jpg'\n",
            "      '0344_c015_00035840_0.jpg'\n",
            "      '0609_c004_00042270_0.jpg'\n",
            "      '0295_c015_00036590_0.jpg'\n",
            "      '0030_c005_00051660_1.jpg'\n",
            "      '0005_c007_00078465_0.jpg'\n",
            "      '0318_c016_00022080_0.jpg'\n",
            "      '0207_c019_00019450_0.jpg'\n",
            "      '0144_c015_00024595_0.jpg'\n",
            "      '0489_c002_00076105_0.jpg'\n",
            "\n",
            "   . jpg files: 11579\n",
            "   .JPG files: 0\n",
            "   .jpeg files: 0\n"
          ]
        }
      ]
    }
  ]
}